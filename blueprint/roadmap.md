# TinyStories LLM Training Roadmap

This roadmap outlines the plan for training and experimenting with various small-scale language models on the TinyStories dataset using LitGPT.

## 1. Project Overview

**Objective**: Train a series of small-scale LLMs (both GPT and LLaMA-style) on the TinyStories dataset for experimentation and mechanical interpretability.

**TinyStories Dataset**:
- Simple stories generated by GPT-3.5/GPT-4 using elementary vocabulary for young children
- Available on Hugging Face: [roneneldan/TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)
- Ideal for small-scale model training and interpretability research

**Key Benefits**:
- Small, easily interpretable models
- Full training possible on consumer hardware
- Simple text domain makes analysis more tractable
- Good benchmark for mechanistic interpretability

## 2. Model Architectures

We'll train models with two different architectures:

### 2.1 LLaMA-Style Models
LLaMA-style architecture using RMSNorm and SwiGLU activations:

| Variant | Layers | Heads | Dim | Head Size | MLP Hidden | Params |
|---------|--------|-------|-----|-----------|------------|--------|
| llama_6L_6H_288D | 6 | 6 | 288 | 48 | 768 | 13.8M |
| llama_8L_8H_768D | 8 | 8 | 768 | 96 | 3072 | 81.2M |
| llama_12L_8H_1024D | 12 | 8 | 1024 | 128 | 4096 | 184.0M |

### 2.2 GPT-Style Models
Classic GPT-style architecture using LayerNorm and standard MLP:

| Variant | Layers | Heads | Dim | Head Size | MLP Hidden | Params |
|---------|--------|-------|-----|-----------|------------|--------|
| gpt_6L_6H_288D | 6 | 6 | 288 | 48 | 768 | 13.8M |
| gpt_8L_8H_768D | 8 | 8 | 768 | 96 | 3072 | 81.2M |
| gpt_12L_8H_1024D | 12 | 8 | 1024 | 128 | 4096 | 184.0M |

### 2.3 Additional Baseline Models (Optional)
For comparative analysis:

| Model | Style | Size | Description |
|-------|-------|------|-------------|
| micro-llama-300M | LLaMA | 300M | Small LLaMA, still tractable |
| pythia-70m | GPT | 70M | Popular for mechanistic interpretability |
| tiny-llama-1.1b | LLaMA | 1.1B | Larger but still trainable locally |

## 3. Implementation Plan

### 3.1 Setup (Week 1)

1. **Environment Setup**:
   - Clone LitGPT repository
   - Install dependencies
   - Configure logging (Weights & Biases or TensorBoard)

2. **Data Preparation**:
   - Download TinyStories dataset
   - Preprocess and tokenize data
   - Verify data loading pipeline functions correctly

3. **Configuration Creation**:
   - Create YAML configuration files for each model variant in `config_hub/pretrain/tiny_stories/`
   - Configure hyperparameters for each model architecture

### 3.2 Training (Weeks 2-4)

1. **Small Model Training (GPT_6L_6H_288D & LLAMA_6L_6H_288D)**:
   - Run initial training on smallest models
   - Validate results and performance
   - Optimize hyperparameters if needed

2. **Medium Model Training (GPT_8L_8H_768D & LLAMA_8L_8H_768D)**:
   - Train medium-sized models
   - Compare performance metrics with small models
   - Adjust learning rate and batch size if needed

3. **Large Model Training (GPT_12L_8H_1024D & LLAMA_12L_8H_1024D)**:
   - Train largest models
   - Monitor training stability and memory usage
   - Employ gradient accumulation if needed for larger models

### 3.3 Evaluation & Analysis (Week 5)

1. **Performance Evaluation**:
   - Measure perplexity on validation set
   - Calculate training efficiency (tokens/sec)
   - Compare convergence rates between architectures

2. **Sample Generation**:
   - Generate story samples from each model
   - Compare quality and coherence
   - Assess how well each architecture captures the domain

3. **Architecture Comparison**:
   - Compare GPT vs. LLaMA style models
   - Analyze training dynamics and convergence behavior
   - Document findings on relative performance

### 3.4 Interpretability Study (Week 6+, Optional)

1. **Attention Pattern Analysis**:
   - Visualize attention patterns across layers
   - Identify key patterns used for story generation
   - Compare attention mechanisms between architectures

2. **Neuron/Feature Analysis**:
   - Identify specific neurons/features that activate on key concepts
   - Compare activation patterns between model sizes
   - Map how conceptual representations evolve through layers

3. **Ablation Studies**:
   - Perform targeted neuron/head ablations
   - Measure impact on model performance
   - Identify critical components in each architecture

## 4. Technical Specifications

### 4.1 Common Configuration Settings

- **Vocabulary Size**: 32,000 tokens
- **Context Length**: 256 tokens for small models (6L_6H_288D), 512 tokens for medium and large models
- **Tokenization Strategy**: Using both BOS and EOS tokens to clearly mark story boundaries
  - BOS token at the beginning of each story helps model learn where stories start
  - EOS token at the end teaches the model to properly conclude stories
- **Batch Size**: Global = 512, Micro = 128 or 512 (device-dependent)
- **Optimizer**: AdamW with weight decay = 0.1
- **Learning Rate**: 0.0005 with cosine decay
- **LR Warmup Steps**: 1,000 steps
- **Weight Tying**: Yes (input embeddings and LM head share weights)
- **Precision**: bf16-mixed (when available)

### 4.2 Hardware Requirements

- **Small Models (13.8M params)**:
  - Trainable on CPU/MPS (M-series Mac)
  - Training time: ~1-2 days on modern CPU

- **Medium Models (81.2M params)**:
  - Recommended: Single GPU with 8GB+ VRAM
  - Training time: ~3-5 days on single GPU

- **Large Models (184M params)**:
  - Recommended: Single GPU with 16GB+ VRAM
  - Training time: ~1 week on single GPU

## 5. Orchestration and Execution

### 5.1 Experiment Tracking

We use a YAML-based experiment tracking system to manage and monitor training runs:

```yaml
experiments:
  - name: gpt_6L_6H_288D
    config: tiny_stories/gpt/6L_6H_288D.yaml
    devices: 1
    environment: local
    status: todo  # todo, running, done, failed
```

The workflow is managed through a Makefile with the following commands:

```bash
# List all experiments and their status
make list

# Run the next pending experiment
make run-next

# Run a specific model
make run-specific MODEL=gpt_6L_6H_288D

# Run all pending experiments in sequence
make run-all
```

All models are configured to automatically resume training from the latest checkpoint if interrupted.

### 5.2 Command Reference

### 5.1 Training Commands

**Direct CLI Command**:
```bash
litgpt pretrain \
  @config_hub/pretrain/tiny_stories/gpt_6L_6H_288D.yaml \
  --devices=1 \
  --num_nodes=1 \
  --logger_name=wandb \
  --resume=false \
  --eval.initial_validation=true \
  --eval.final_validation=true \
  --out_dir=experiments/tiny_stories/gpt_6L_6H_288D/logs/$(date +%Y%m%d_%H%M%S).log
```

**Alternative CLI Command for LLaMA models**:
```bash
litgpt pretrain \
  @config_hub/pretrain/tiny_stories/llama_6L_6H_288D.yaml \
  --devices=1 \
  --num_nodes=1 \
  --logger_name=wandb \
  --resume=false \
  --eval.initial_validation=true \
  --eval.final_validation=true \
  --out_dir=experiments/tiny_stories/llama_6L_6H_288D/logs/$(date +%Y%m%d_%H%M%S).log
```

### 5.2 Configuration Examples

**GPT Configuration Example (`config_hub/pretrain/tiny_stories/gpt_6L_6H_288D.yaml`)**:
```yaml
model_config:
  name: gpt_6L_6H_288D
  block_size: 256
  padded_vocab_size: 32000
  n_layer: 6
  n_head: 6
  n_embd: 288
  head_size: 48
  rotary_percentage: 1.0
  parallel_residual: false
  bias: false
  norm_class_name: LayerNorm
  mlp_class_name: GptNeoxMLP
  intermediate_size: 768

out_dir: experiments/tiny_stories/gpt_6L_6H_288D

data: TinyStories

train:
  save_interval: 1000
  log_interval: 1
  global_batch_size: 512
  micro_batch_size: 128
  lr_warmup_steps: 1000
  max_tokens: 9700000000
  max_seq_length: 256
  tie_embeddings: true
  max_norm: 1.0
  min_lr: 0.0

eval:
  interval: 2000
  max_iters: 100
  initial_validation: true
  final_validation: true

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0005
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95

precision: bf16-mixed
logger_name: wandb
seed: 42
```

## 6. Next Steps and Future Work

- Implement fine-tuning on similar datasets
- Experiment with different attention mechanisms
- Add speculative decoding for faster inference
- Compare with other small model families (Phi, Pythia)
- Open-source trained models on Hugging Face Hub

## 7. Milestones and Deliverables

### Milestone 1: Initial Setup and Small Model Training
- Dataset preparation complete
- Small GPT and LLaMA models trained
- Initial evaluation metrics documented

### Milestone 2: Medium and Large Model Training
- All model variants trained
- Performance comparisons between sizes
- Generated samples collected

### Milestone 3: Analysis and Comparison
- Comprehensive analysis of model behaviors
- Architecture comparison report
- Trained model weights released

### Milestone 4 (Optional): Interpretability Studies
- Attention visualization tools
- Feature analysis documentation
- Ablation study results
